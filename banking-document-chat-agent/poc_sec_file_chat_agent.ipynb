{
  "cells": [
    {
      "cell_type": "code",
      "execution_count": 16,
      "metadata": {
        "id": "7I4u9JjTlA2T"
      },
      "outputs": [],
      "source": [
        "!pip install -qU pinecone-client openai langchain gradio"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 17,
      "metadata": {
        "id": "xlRzKza_lI6N"
      },
      "outputs": [],
      "source": [
        "import openai\n",
        "import warnings\n",
        "import pinecone\n",
        "import warnings\n",
        "\n",
        "from langchain.embeddings.openai import OpenAIEmbeddings\n",
        "from langchain.agents import initialize_agent\n",
        "from langchain.chat_models import ChatOpenAI\n",
        "from langchain.vectorstores import Pinecone\n",
        "from langchain.chains.conversation.memory import ConversationBufferWindowMemory\n",
        "from langchain.chains import RetrievalQA\n",
        "from langchain.agents import Tool\n",
        "from langchain.agents import initialize_agent"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "yvQt6bqpl2zN"
      },
      "outputs": [],
      "source": [
        "openai.api_key = \"\"\n",
        "PINECONE_API_KEY = \"\"  # find next to api key in console\n",
        "PINECONE_ENV = \"\"  # find next to api key in console\n",
        "\n",
        "llm = ChatOpenAI(openai_api_key=openai.api_key, temperature=0.0)\n",
        "\n",
        "EMBEDDING_MODEL_NAME = 'text-embedding-ada-002'\n",
        "warnings.filterwarnings('ignore')\n",
        "\n",
        "embed = OpenAIEmbeddings(\n",
        "    model=EMBEDDING_MODEL_NAME,\n",
        "    openai_api_key=openai.api_key\n",
        ")\n",
        "\n",
        "index_name = 'semantic-search-openai'\n",
        "pinecone.init(\n",
        "    api_key=PINECONE_API_KEY,\n",
        "    environment=PINECONE_ENV\n",
        ")\n",
        "\n",
        "if index_name not in pinecone.list_indexes():\n",
        "    # we create a new index\n",
        "    pinecone.create_index(\n",
        "        name=index_name,\n",
        "        metric='dotproduct',\n",
        "        dimension=1536  # 1536 dim of text-embedding-ada-002\n",
        "    )\n",
        "# connect to index\n",
        "index = pinecone.Index(index_name)\n",
        "print('Pinecone index status is', index.describe_index_stats())\n",
        "\n",
        "text_field = \"text\"\n",
        "vectorstore = Pinecone(\n",
        "    index, embed.embed_query, text_field\n",
        ")\n",
        "\n",
        "# conversational memory\n",
        "conversational_memory = ConversationBufferWindowMemory(\n",
        "    memory_key='chat_history',\n",
        "    k=1,\n",
        "    return_messages=True\n",
        ")\n",
        "# retrieval qa chain\n",
        "qa = RetrievalQA.from_chain_type(\n",
        "    llm=llm,\n",
        "    chain_type=\"stuff\",\n",
        "    retriever=vectorstore.as_retriever()\n",
        ")\n",
        "\n",
        "tools = [\n",
        "    Tool(\n",
        "        name='Knowledge Base',\n",
        "        func=qa.run,\n",
        "        description=(\n",
        "            'use this tool when answering queries about '\n",
        "            'the SEC files'\n",
        "        )\n",
        "    )\n",
        "]\n",
        "\n",
        "agent = initialize_agent(\n",
        "    agent='chat-conversational-react-description',\n",
        "    tools=tools,\n",
        "    llm=llm,\n",
        "    verbose=True,\n",
        "    max_iterations=3,\n",
        "    early_stopping_method='generate',\n",
        "    memory=conversational_memory\n",
        ")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 20,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "ecaEdIj2mQ9u",
        "outputId": "d4d227ce-8d8d-476e-bacb-e1b64941ce04"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "\n",
            "\n",
            "\u001b[1m> Entering new AgentExecutor chain...\u001b[0m\n",
            "\u001b[32;1m\u001b[1;3m{\n",
            "    \"action\": \"Knowledge Base\",\n",
            "    \"action_input\": \"What is the total revenue of Tesla for 2022?\"\n",
            "}\u001b[0m\n",
            "Observation: \u001b[36;1m\u001b[1;3mThe total revenue of Tesla for 2022 was $81.46 billion, representing an increase of $27.64 billion compared to the prior year.\u001b[0m\n",
            "Thought:\u001b[32;1m\u001b[1;3m{\n",
            "    \"action\": \"Final Answer\",\n",
            "    \"action_input\": \"The total revenue of Tesla for 2022 was $81.46 billion, representing an increase of $27.64 billion compared to the prior year.\"\n",
            "}\u001b[0m\n",
            "\n",
            "\u001b[1m> Finished chain.\u001b[0m\n",
            "{'input': 'Whats the total revenue of Tesla for 2022?', 'chat_history': [], 'output': 'The total revenue of Tesla for 2022 was $81.46 billion, representing an increase of $27.64 billion compared to the prior year.'}\n"
          ]
        }
      ],
      "source": [
        "print(agent(\"Whats the total revenue of Tesla for 2022?\"))"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 21,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 618
        },
        "id": "f-2oofVAmxZf",
        "outputId": "841ab7cf-d9f4-4d67-f775-230de6c0f170"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Colab notebook detected. To show errors in colab notebook, set debug=True in launch()\n",
            "Note: opening Chrome Inspector may crash demo inside Colab notebooks.\n",
            "\n",
            "To create a public link, set `share=True` in `launch()`.\n"
          ]
        },
        {
          "data": {
            "application/javascript": "(async (port, path, width, height, cache, element) => {\n                        if (!google.colab.kernel.accessAllowed && !cache) {\n                            return;\n                        }\n                        element.appendChild(document.createTextNode(''));\n                        const url = await google.colab.kernel.proxyPort(port, {cache});\n\n                        const external_link = document.createElement('div');\n                        external_link.innerHTML = `\n                            <div style=\"font-family: monospace; margin-bottom: 0.5rem\">\n                                Running on <a href=${new URL(path, url).toString()} target=\"_blank\">\n                                    https://localhost:${port}${path}\n                                </a>\n                            </div>\n                        `;\n                        element.appendChild(external_link);\n\n                        const iframe = document.createElement('iframe');\n                        iframe.src = new URL(path, url).toString();\n                        iframe.height = height;\n                        iframe.allow = \"autoplay; camera; microphone; clipboard-read; clipboard-write;\"\n                        iframe.width = width;\n                        iframe.style.border = 0;\n                        element.appendChild(iframe);\n                    })(7863, \"/\", \"100%\", 500, false, window.element)",
            "text/plain": [
              "<IPython.core.display.Javascript object>"
            ]
          },
          "metadata": {},
          "output_type": "display_data"
        },
        {
          "data": {
            "text/plain": []
          },
          "execution_count": 21,
          "metadata": {},
          "output_type": "execute_result"
        }
      ],
      "source": [
        "import gradio as gr\n",
        "\n",
        "def query_vector_db(query):\n",
        "    return agent(query)\n",
        "\n",
        "sec_file_query_demo_app = gr.Interface(\n",
        "    fn=query_vector_db,\n",
        "    inputs=gr.Textbox(lines=1, placeholder=\"Please Enter the SEC file search query here !\"),\n",
        "    outputs=\"text\",\n",
        ")\n",
        "sec_file_query_demo_app.launch()"
      ]
    }
  ],
  "metadata": {
    "colab": {
      "provenance": []
    },
    "kernelspec": {
      "display_name": "Python 3",
      "name": "python3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 0
}
