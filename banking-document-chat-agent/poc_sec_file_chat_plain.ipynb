{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": []
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "avOV_5wVgl2p"
      },
      "outputs": [],
      "source": [
        "!pip install -qU pinecone-client openai langchain gradio"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "import gradio as gr\n",
        "import pinecone\n",
        "import openai\n",
        "import warnings\n",
        "\n",
        "from langchain.vectorstores import Pinecone\n",
        "from langchain.embeddings.openai import OpenAIEmbeddings\n",
        "from langchain.chat_models import ChatOpenAI\n",
        "from langchain.chains import RetrievalQA\n",
        "\n",
        "openai.api_key = \"\"\n",
        "PINECONE_API_KEY = \"\"  # find next to api key in console\n",
        "PINECONE_ENV = \"\"  # find next to api key in console\n",
        "\n",
        "EMBEDDING_MODEL_NAME = 'text-embedding-ada-002'\n",
        "warnings.filterwarnings('ignore')\n",
        "\n",
        "embed = OpenAIEmbeddings(\n",
        "    model=EMBEDDING_MODEL_NAME,\n",
        "    openai_api_key=openai.api_key\n",
        ")\n",
        "\n",
        "index_name = 'semantic-search-openai'\n",
        "pinecone.init(\n",
        "    api_key=PINECONE_API_KEY,\n",
        "    environment=PINECONE_ENV\n",
        ")\n",
        "\n",
        "if index_name not in pinecone.list_indexes():\n",
        "    # we create a new index\n",
        "    pinecone.create_index(\n",
        "        name=index_name,\n",
        "        metric='dotproduct',\n",
        "        dimension=1536  # 1536 dim of text-embedding-ada-002\n",
        "    )\n",
        "# connect to index\n",
        "index = pinecone.Index(index_name)\n",
        "print('Pinecone index status is', index.describe_index_stats())\n",
        "\n",
        "text_field = \"text\"\n",
        "vectorstore = Pinecone(\n",
        "    index, embed.embed_query, text_field\n",
        ")\n",
        "\n",
        "MODEL_NAME = 'gpt-3.5-turbo'\n",
        "# completion llm\n",
        "llm = ChatOpenAI(\n",
        "    openai_api_key=openai.api_key,\n",
        "    model_name=MODEL_NAME,\n",
        "    temperature=0.0\n",
        ")\n",
        "\n",
        "qa = RetrievalQA.from_chain_type(\n",
        "    llm=llm,\n",
        "    chain_type=\"stuff\",\n",
        "    retriever=vectorstore.as_retriever()\n",
        ")"
      ],
      "metadata": {
        "id": "NxnxbNt1g47Q"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "Now test the SEC file reader"
      ],
      "metadata": {
        "id": "d83_dtxHkpuI"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "print(qa(\"What is the revanues for Tesla for 2022?\"))"
      ],
      "metadata": {
        "id": "C1gUAlgLh6ZF"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "Build a Web UI using Gradio!!"
      ],
      "metadata": {
        "id": "aePck7XokmN5"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "import gradio as gr\n",
        "\n",
        "def query_vector_db(query):\n",
        "    return qa(query)\n",
        "\n",
        "sec_file_query_demo_app = gr.Interface(\n",
        "    fn=query_vector_db,\n",
        "    inputs=gr.Textbox(lines=1, placeholder=\"Please Enter the SEC file search query here !\"),\n",
        "    outputs=\"text\",\n",
        ")\n",
        "sec_file_query_demo_app.launch()"
      ],
      "metadata": {
        "id": "Hd2gzOSFia7n"
      },
      "execution_count": null,
      "outputs": []
    }
  ]
}